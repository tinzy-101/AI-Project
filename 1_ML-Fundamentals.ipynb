{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fbad93e-b7dc-4fe1-a1d8-3ca6ec807fcb",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "Typical workflow: identify problem, analyze problem, create detection algo for problem, program can flag issue areas, repeat until good enough to launch!\n",
    "ML also good for problems w/ no algo to solve + can help humans to learn by showing us patterns found thru large amounts of data (data mining)\n",
    "ML good at adapting to new environments, so can analyze fluctuating data too \n",
    "\n",
    "Applications:\n",
    "1. image classification (CNNS, ch14)\n",
    "2. detecting tumors in brain scans (Semantic segmentation, CNNs, ch14)\n",
    "3. classify news articles (NLP, ch16)\n",
    "4. forecasting revenue (regression, ch4, ch 10, ch 15/16)\n",
    "5. react to voice commands (voice detection, ch15/16)\n",
    "6. detect credit card fraud (anomaly detection, ch9)\n",
    "7. segment clients based on purchasing patterns (clustering, ch9)\n",
    "8. represet complex n-dim dataset clearly in diagram (ch 8)\n",
    "9. recommend product for client based on past purchased (artificial neural network, ch 10)\n",
    "10. intelligent bot for game (reinforcement learning, ch 18)\n",
    "\n",
    "### Type of ML Systems\n",
    "1. trained w/ human supervisions or not (supervised, unsupervised, semisupervised, reinforcement learning)\n",
    "2. can learn incrementally (online vs batch learning)\n",
    "3. work by comparing new data to old OR detect patterns in training data then build predictive model (instance-based vs\n",
    "\n",
    "Can combine multiple criteria!\n",
    "\n",
    "#### Supervised/unsupervised Learning\n",
    "1. supervised learning\n",
    "    - feed algorithm desired solutions (\"labels\")\n",
    "    - e.g. classification, predicting a target numeric value (regression, needs many example sof predictors and their labels)\n",
    "    - important algos:\n",
    "       - k-nearest neighbors\n",
    "       - linear regression\n",
    "       - logistic regression\n",
    "       - support vector machines (SVMs)\n",
    "       - decision trees and random forests\n",
    "       - neural networks\n",
    "\n",
    "2. unsupervised learning\n",
    "    - training data is unlabeled (not given any solutions)\n",
    "    - important algos:\n",
    "          - Clustering\n",
    "              - k-means\n",
    "              - DBSCAN\n",
    "              - hierarchical cluster analysis (HCA)\n",
    "          - anomaly deteciton and novelty detection\n",
    "                  - system is trained to see normal instances, so when it sees new instance, can tell if it looks like normal\n",
    "              - one-class SVM\n",
    "              - isolation forest\n",
    "          - visualization and dimensionality reduction\n",
    "                  - visualization: feed lots of complex data, and can output 2d/3d representation ot be plotted, try not to simplify\n",
    "                  - dim red: simplify data w/o losing too much info, e.g. combine stuff\n",
    "                      - good to use dim red b4 feeding data to another ML algo \n",
    "              - principal component analysis (PCA)\n",
    "              - kernel PCA\n",
    "              - locally linear embedding (LLE)\n",
    "              - t-distributed stochastic neighbor embedding (t-SNE)\n",
    "          - association rule learning\n",
    "                  - find unique relations in a lot of data \n",
    "              - apriori\n",
    "              - eclat\n",
    "\n",
    "3. semisupervised learning\n",
    "    - labeling data is very costly/time consuming, so have only a few labeled and most unlabeled\n",
    "    - e.g. google photos automatically recognizes same person showing up in multiple photos (unsupervised algo)\n",
    "\n",
    "4. reinforcement learning\n",
    "    - very diff from (un)(semi)supervised learning\n",
    "        - learning system = agent\n",
    "        - agent observed environment + performs actions + can get rewards or penalties\n",
    "        - agent needs to learn by itself what is the best policy/strategy to get most reward\n",
    "    - e.g. alpha go analyzed games and played by itself\n",
    "  \n",
    "\n",
    "### Batch/online Learning\n",
    "1. Batch learning\n",
    "    - system cannot learn incrementally, needs to be trained w/ all avail data\n",
    "    - usually train offline b/c takes a long time\n",
    "    - if want batch system to know about new data, need to train new system from scratch on WHOLE dataset, then replace old sys\n",
    "       - can be automated so not so bad\n",
    "    - cons:\n",
    "        - lots of computing power (CPU, mem, i/o)\n",
    "        - not good for rapidly changing data sets\n",
    "2. Online learning\n",
    "    - train system incrementally by feeding it data instances sequentially (individual or mini batch)\n",
    "    - fast and cheap for each step\n",
    "    - can also train systems on huge datasets that can't fit onto machine main mem (can work thru it incrementally)\n",
    "    - learning rate = how fast sys adapts to changing data\n",
    "          - high learning rate = high turnover (learn new quickly, forget old quickly)\n",
    "          - slow learning rate = more intertia, less sensitive to noise in new data or outliers\n",
    "    - con:\n",
    "          - if bad data, performance goes down, clients notice (need fast response times)\n",
    "\n",
    "\n",
    "### Instance Based/Model-Based Learning\n",
    "1. instance learning\n",
    "    - create measure of similarity between group that you want to flag, and group that is normal and doesn't need to be flagged, e.g. similar number of words\n",
    "    - system will learn examples by heart and can generalize new cases w/ the similarity measure to compare to cases it knows the answer to\n",
    "\n",
    "2. Model-based learning\n",
    "    - build model using examples + use model to make prediction (way to generalize from set of examples)\n",
    "    - even with noisy data, can still generalize the data by fitting it with a best-fit model, e.g. linear\n",
    "    - will have model parameters, tweak params to make model repesent any linear function\n",
    "          - need to define params b4 using model\n",
    "          - specify performance measure: ultility function (Define how good model is) OR cost function (how bad)\n",
    "              - linear regression = usually use cost\n",
    "              - performance measure helps you find out what to set params to for best performance\n",
    "    - linear regression algo fed training examples, algo will find params to make linear model fit best to data (training model)\n",
    "        - training model = run algo to find params for model for it to be a best fit for training data\n",
    "    - now that figure out params, can run model to make predictions\n",
    "    - if after running model, the model doesn't make good predictions, options:\n",
    "          - use more attributes (e.g. employment rate as factor affecting gdp, instead of only looking at exports)\n",
    "          - get better quality training data\n",
    "          - get more powerful model (e.g. polynomial regression model)\n",
    "\n",
    "\n",
    "### Main Challenges of ML \n",
    "1. Insufficient quantity of training data\n",
    "    - even for simple problems need thousands of examples\n",
    "    - need millions of example for complex (e.g. image, speech recognition) unless can reused parts of existing model\n",
    "      \n",
    "2. Nonrepresentative Training data\n",
    "    - training data needs to correlate to data you want model to be tested on (no matter instance or model learning)\n",
    "    - e.g. if missing a few countries when finding model of GDP and life satisfaction, then linear line model is skewed and gives inaccurate predictions\n",
    "    - training set NEEDS to represent cases you want to generalize to\n",
    "          - set too small = sampling noise (nonrepresentative data b/c of chance)\n",
    "          - set too big = sampling bias (nonrep if sampling method bad)\n",
    "\n",
    "3. Poor quality data\n",
    "    - data with errors, outliers, noise (bad measurements) = hard to detect patterns\n",
    "    - need to clean up training data (throw away bad ones or manually clean)\n",
    "    - if many samples missing an attribute (e.g. most ppl don't fill out age on survey, then decide if want to ignore the attribute, or fill it w/ medium value, or train one model with the attrib and w/o\n",
    "  \n",
    "4. Irrelevant Features\n",
    "    - sys can only learn if training data have enough relevant features, not too many irrelevant ones\n",
    "    - need feature engineering: selectin useful features to train on, combine existing features to create more useful one (e.g. us dim red algos), create new features by gathering data\n",
    "\n",
    "4. Overfitting the Training Data\n",
    "    - bad to overgeneralize (\"overfit\"), e.g. if 1 Canadian rips you off, doesn't necessarily mean that ALL canadians will rip you off\n",
    "    - e.g. polynomial might fit data strongly, but not as good as predictor as linear model\n",
    "    - complex models, e.g. neural networks, can detect small patterns in data (e.g. find patterns in noise)\n",
    "        - e.g. just so happens that many countries w/ highest GDP have \"w\" in name != that having \"w\" means high GDP\n",
    "        - model doesn't know if the \"w\" rule happened by chance from noise, or if it is a meaningful pattern\n",
    "    - solutions to overfitting:\n",
    "        1. simplify model w/ less params, less attributes in training data, or constraining model\n",
    "            - \"regularization\"\n",
    "                - 2 params = 2 degrees of freedom\n",
    "                - if set params equal to each other, only 1 degree of freedom: simplified!\n",
    "                - if allow algo to modify one param, but force keep it small, then algo has b/w 1 and 2 degrees of freedom: simplified!\n",
    "            - control amt of regularization using hyperparam = param of learning algo (NOT of model)\n",
    "                - won't be affected by learning sys + need to be set b4 training to keep model in check\n",
    "                - large hyperparam = flat model (slope close to 0) --> not good\n",
    "        2. more training data\n",
    "        3. less noise in training data (fix data errors and remove outliers)\n",
    "     \n",
    "6. Underfitting the Training Data\n",
    "    - model too simple to match unerlying structure of data\n",
    "    - solutions:\n",
    "        1. powerful model more\n",
    "        2. better features to learning algo\n",
    "        3. reduce constraints on model (e.g. lower regularization hyperparam)\n",
    "     \n",
    "### Testing and Validating\n",
    "Need to try out model on new cases and monitor how it does \n",
    "    - need to split data into training set + test set (80%, 20%)\n",
    "    - generalization error = error rate on new cases \n",
    "    - low training error, but high generalization error = overfitting training data \n",
    "\n",
    "#### Hyperparameter Tuning and Model Selection\n",
    "1. how to decide between 2 models: train and compare both to how well generalize using test set \n",
    "    1. if linear model generalize better: \n",
    "        - apply regularization so not overfitting\n",
    "        - how to choose regularization hyperparam? could train 100 diff models with 100 diff hyperparams \n",
    "        - eventually find one hyperparam value where generalization error least \n",
    "        - not good!!! will not work well on new data, gives higher generalization error than predicted \n",
    "        - solution: \n",
    "            - holdout validation = hold out part of training set to evaluate candidate models, then choose best one \n",
    "            - validation set = data that's held out to train other models to get best hyperparameter value \n",
    "            - once you get the model w/ best hyperparam value , train best model on FULL training set (including validation set) \n",
    "            - summary:\n",
    "                - data set --> validation test --> test 100 models w/ diff hyperparameterization values --> choose  with lowest generalization error --> train that model with the WHOLE set of data to estimate generalization error\n",
    "        - problems with solution:\n",
    "            - validation set too small, and model evaluations inaccurate\n",
    "            - validation set too large, and not enough remaining data to train final model\n",
    "        - solution to problem:\n",
    "            - use cross validation: e.g. many small validation sets, every model evaluated once per validation set after trained on rest of data\n",
    "         \n",
    "#### Data Mismatch\n",
    "1. sometimes easy to get a lot of data, but data wont represent all data used in production\n",
    "2. **most important!!!** make sure validation set is representative of data used in production\n",
    "3. if train model and result bad, idk if problem is with overfitting training set, or bc mismatch of data\n",
    "    - solution:\n",
    "        - after model trained on training set, evaluate with train-dev set (set of held out training pictures)\n",
    "        - if does poorly on validation set, then problem is from data mismatch\n",
    "            - can try to constrain model\n",
    "            - can try to retrain model using more representative data\n",
    "        - if does poorly on train-dev set, thne overfit data\n",
    "            - simplify or regularize model, get more training data, clean up training data\n",
    "4. No Free Lunch Theorem: no modle is a priori guaranteed to work better (all have their own tradeoffs)\n",
    "    - just have to make reasonable assumptions about data + evaluate only few reasonable models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16be23f0-cc76-45bc-8442-70733e862931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
